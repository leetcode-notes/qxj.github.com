---
title: Explore & Exploit
tags: ReinforcementLearning
---

参考 《计算广告学》刘鹏 6.3 探索与利用
参考 《机器学习》周志华 16.2 K-摇臂赌博机

----

## bindit算法

https://zhuanlan.zhihu.com/p/21388070

一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么想最大化收益该怎么整？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB ）。

怎么解决这个问题呢？求菩萨？拜赌神？都不好使，最好的办法是去试一试，而这个试一试也不是盲目地试，而是有策略地试，越快越好，这些策略就是bandit算法。

特别提出，在计算广告和推荐系统领域，针对这个问题，还有个说法叫做EE问题：exploit－explore问题。

- exploit意思就是：比较确定的兴趣，当然要用啊。好比说我们已经挣到的钱，当然要花啊；
- explore意思就是：不断探索用户新的兴趣才行，不然很快就会出现一模一样的反复推荐。就好比我们虽然有一点钱可以花了，但是还得继续搬砖挣钱啊，不然花完了喝西北风啊。

### Cumulative Expected Regret

累积后悔，用来评估bandit算法。

多arm的回报可以看作一组真实的随机分布：$B=( R_i,\dots,R_K )$，其中，$K$代表arm。

设 $\mu_1,\dots,\mu_K$ 代表每个arm的平均回报，每一轮拉动一个arm得到一次回报。多arm问题等同于一个状态的马尔科夫问题。

设$T$轮之后的后悔度为 $\rho$，则可以表示如下：

$$
\rho = T\mu^* - \sum_{t=1}^T \hat{r_t}
$$

其中，$\mu^*=\max_k( \mu_k )$ 是最大回报的平均数，$\hat{r_t}$ 是在t时获得的回报。

一个零后悔度的策略会使得 $\rho/T$在无限次选择arm后以概率1趋向于0。

## Thompson sampling

## Epsilon-Greedy

利用概率ε来探索，概率1-ε来开发。

## UCB

置信区间上界（Upper Confidence Bound）

在统计学中，对于一个未知量的估计，总能找到一种量化其置信度的方法。最普遍的分布正态分布 N(μ,δ)，其中的μ就是估计量的期望，而δ则表示其不确定性（δ越大则表示越不可信）。

而UCB就是以arm均值 μ 的置信上限为来代表它的预估值：

$$
\hat{\mu_i} = \hat{\mu_i} +2 \sqrt{\frac1{n_i}}
$$

其中，$\mu_i$ 是对期望的预估，$n_i$是尝试次数，可以看到对i的尝试越多，其预估值与置信上限的差值就越小。也就是越有置信度。

参考实现：https://github.com/jkomiyama/banditlib/blob/master/policy/policy_ucb.hpp

### arm的数据结构

包括拉动的计数和该arm的预估值：

```
struct UCBArm
{
  int count;
  double value;
};
```

### 选择arm的算法

1. 如果有count=0的arm（从未选择过），那么优先选择（即初始时每个臂都会被选择一次）；
2. 如果所有arm都被选择过，则计算每个arm的bonus和value的和，bonus计算公式：`bonus =sqrt(2*log(total_count)) / count` 。
3. 选择bonus+value最大的arm。

因为每一个臂都需要至少被选择一次，因此，在使用UCB算法时需要注意，如果选择的次数M小于总的臂数N，就要谨慎使用UCB算法了。

bonus的意义在于，如果我们对一个臂的了解过于少，因此它的value值在此时的置信度是很低的，所以我们需要选择这个臂来获取更多的信息。因此，bonus可以当做一个测量对臂了解多少的指标，了解越少，bonus越大。加入了bonus这个指标，我们可以说这个算法是有好奇心的，当对于一个臂的了解少于了下限，它会被选中，即使这个臂的回报率很低。

### 更新arm的算法

每个arm被选中之后，会返回一个value，那么更新的方法如下：

1. 该arm的count++
2. 该arm的value变为原有的value和新返回的结果按比例相加： `value=value*(n-1)/n + result/n`。

## LinUCB


----

EOF
