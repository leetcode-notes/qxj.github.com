---
title: 特征值分解和主成份分析
tags: ml math
---

$\newcommand{\cov}{\operatorname{cov}}
\newcommand{\var}{\operatorname{var}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\L}{\mathbf{\Lambda}}
\newcommand{\l}{\mathbf{\lambda}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\S}{\mathbf{S}}
\newcommand{\Q}{\mathbf{Q}}$

## 协方差矩阵

[协方差](http://en.wikipedia.org/wiki/Covariance)(Covariance)用于衡量两个变量的总体误差。设两个随机变量 $X$ 和 $Y$ 的期望值分别为  $E(X)$ 和 $E(Y)$ ，则其协方差定义为：

$$
\cov(X, Y) = E((X - E(X)) (Y - E(Y)))
$$

方差是协方差的一种特殊情况，即当两个变量是相同时：

$$
\var(X) = \operatorname{cov}(X, X) = E((X - E(X))^2)
$$

协方差从随机标量推广到随机向量，则得到[协方差矩阵](http://en.wikipedia.org/wiki/Covariance_matrix)(Convariance Matrix)定义：

$$
\cov(X) = E((X-E(X))(X-E(X))\tr)
$$

其中， $X$ 是一个随机向量。显然一个随机向量的协方差矩阵是一个方阵。

## 线性变换、特征值和特征向量

[线性变换](https://en.wikipedia.org/wiki/Linear_map)(线性映射)是在作用于两个 *向量空间* 之间的函数，它保持向量加法和标量乘法的运算。实际上线性变换表现出来的就是一个矩阵。

[特征值和特征向量](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)是一体的概念：

> 对于一个给定的线性变换，它的特征向量  $\xi$  经过这个线性变换之后，得到的新向量仍然与原来的  $\xi$  保持在同一條直線上，但其长度也许會改变。一个特征向量的长度在该线性变换下缩放的比例称为其特征值（本征值）。

数学描述如下：

$$
\A \xi = \lambda \xi
$$

在 线性变换 $\A$  的作用下，向量 $\xi$  仅仅在尺度上变为原来的  $\lambda$ 倍。称  $\xi$  是线性变换 $\A$  的一个特征向量， $\lambda$ 是对应的特征值。

求解线性变换 $\A$ 的特征向量和特征值：

$$
\begin{align}
\A x &= \lambda x \\
\I\A x &=\I \cdot \lambda x \\
(\A - \lambda \I) x &= 0
\end{align}
$$

<!-- Ax = λx  ⇒ I·Ax = I·λx ⇒ Ax = (λI)x ⇒ (A - λI)x = 0 -->

根据线性方程组理论，如果上式有非零解，则矩阵  $(\A - \lambda \I)$  的行列式为0：

$$
|\A - \lambda \I| = 0
$$

该方程组称作矩阵的[特征多项式](http://en.wikipedia.org/wiki/Characteristic_polynomial)，解该方程组可以求得所有的特征值 $\lambda$ 。矩阵 $\A$ 的非零特征值最大数目是该矩阵的秩 $\operatorname{rank}(\A)$ 。对于每个特征值 $\lambda_i$ 都有如下特征方程(Characteristic equation)成立：

$$
(\A - \lambda_i \I) x  = 0
$$

进一步可以解得相应的特征向量 $x$ 。

顾名思义，特征值和特征向量表达了一个线性变换的特征。在物理意义上，一个高维空间的线性变换可以想象是在对一个向量在各个方向上进行了不同程度的变换，而特征向量之间是 *线性无关* 的，它们对应了最主要的变换方向，同时特征值表达了相应的变换程度。

## 特征值分解

*矩阵对角化定理*(Matrix diagonalization theorem)：对于 $N \times N$ 方阵 $\A$ ，如果它有 $N$ 个线性无关的特征向量，那么存在一个[特征分解](http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)：

$$
\A = \Q\L\Q^{-1}
$$

其中， $\Q$ 是 $N \times N$ 的方阵，且其第 $i$ 列为 $\A$ 的特征向量 $$\mathrm{q}_i$$ 。 $\L$ 是对角矩阵，其对角线上的元素为对应的特征值，即 $$\L_{ii}=\l_i$$ 。

*对称对角化定理*(Symmetric diagonalization theorem)：更进一步，如果方阵 $\A$ 是对称方阵，可得 $\Q$ 的每一列都是 $\A$ 的互相正交且归一化（单位长度）的特征向量，即  $\Q^{-1} = \Q\tr$ 。

## 主成份分析

主成份分析(PCA, Principal Component Analysis)有多种推导方法，最大化方差是一种比较直观的方法。比如给出一坨数据，如果你想给出一条坐标轴可以尽量清晰的描述这些数据，即更容易把它们分类，那么直观来看，肯定会选择与数据方差最大的那条直线，才能最大化数据的差异性。

实际的做法就是将数据的高维坐标投影到这条直线，也就是向量上去，然后最大化投影后的方差。

首先定义原始数据的中心点：

$$
\bar{x} = \frac{1}{N} \sum_{n=1}^N x_n
$$

定义投影向量 $u$ ，不失一般性，可以令 $u\tr u=1$ 。每个数据点 $x_n$ 投影之后的的值为 $u x_n$ ，投影之后的方差可表示为：

$$
\frac{1}{N} \sum_{n=1}^N (u\tr x_n - u\tr\bar{x})^2 = u\tr \S u
$$

其中， $\S$ 是数据的协方差矩阵。现在需要最大化投影方差  $u\tr \S u$ ，利用微积分中常用的求极值方法[拉格朗日乘子法](http://en.wikipedia.org/wiki/Lagrange_multiplier)，引入标量 $\lambda$ 变为求下式的极值：

$$
u\tr \S u + \lambda(1-u\tr u)
$$

求导，取零，可得：

$$
\S u = \lambda u
$$

很明显上式的含义是 $u$ 是矩阵 $\S$ 的特征向量，这就转换成一个矩阵特征值分解问题。我们将特征值从大到小排列，保留前M个特征向量，即实现了从原来N维空间到M维新空间的降维。

## 参考

- PRML 12.1. Principal Component Analysis
- Introduction to IR 18. Dimensionality reduction and latent semantic indexing
