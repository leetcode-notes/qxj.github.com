---
title: 高斯混合模型
tags: ml clustering
---

高斯混合模型(GMM, Gaussian Mixture Model)是[混合模型](http://en.wikipedia.org/wiki/Mixture_model)的一种，它可以和k-means算法一样用于聚类，但k-means是把数据直接划分到某个cluster，而GMM则可以进一步给出该数据点可以被划分到该cluster的概率。

给定 $N$ 个训练样本 $\brace{x^{(1)},\dots,x^{(N)}}$ 。同样，假设样本集合可以被分成 $K$ 类(又称component)，即 $x^{(i)}$ 分到相应类别 $z^{(i)} \in \brace{1,\dots,K}$ ，并且样本属于某个类别的概率服从多项式分布 $z^{(i)} \sim Multinomial(\phi)$ ，其中 $p(z^{(i)}=j)=\phi_j, \phi_j \geq 0, \sum_{j=1}^K \phi_j=1$ ；而每个类别里的样本分布服从高斯分布 $x^{(i)}\vert z^{(i)}=j \sim \mathcal{N}(\mu_j, \Sigma_j)$ 。由于各个类别里的样本分布可以表示为  $p(x^{(i)},z^{(i)}) = p(z^{(i)})p(x^{(i)}\vert z^{(i)})$ ，可得样本分布的概率密度函数：

$$
\begin{align}
p(x^{(i)}) &= \sum_{z^{(i)}=1}^K p(z^{(i)})p(x^{(i)}|z^{(i)}) \\
    &= \sum_{z^{(i)}=1}^K p(z^{(i)};\phi)p(x^{(i)}|z^{(i)};\mu,\Sigma)
\end{align}
$$

已知分布的概率密度函数，求解函数参数，这是一个参数估计问题，我们常用的方法是最大似然估计。但这里跟以前我们所知的参数估计又有不同，变量 $x^{(i)}$ 是我们可以观察到的样本数据，而变量 $z^{(i)}$ 我们观察不到，被称作*隐变量*(latent variable)，这使得我们无法使用之前对函数求导取极值的办法直接求解参数。我们需要使用[EM算法](http://en.wikipedia.org/wiki/Expectation-maximization_algorithm)(Expectation Maximization)迭代求解，这也是针对含有隐变量的概率模型的参数估计的通用求解方法。不过EM算法的推导还是比较复杂的，这里借助k-means算法的思想，可以更加直观的类比得到同等效果的估计方法。

同样的，类似最大似然估计方法，我们先写出高斯混合模型的对数似然函数：

$$
\ell(x^{(i)}; \phi, \mu, \Sigma) = \sum_{i=1}^N \ln \brace{ \sum_{k=1}^K \phi_k \mathcal{N}(x^{(i)}; \mu_k, \Sigma_k) }
$$

类比k-means迭代中的两步：

1.  寻找样本所属质心的相应类别 ⇒ 计算样本 $x^{(i)}$ 属于类别 $k$ 的概率 $\gamma(i,k)$ 。这里我们假设分布已知，即 $\mu_k, \Sigma_k$ 已知，取随机初始值或上一次迭代所得的值：

    $$
    \gamma(i,k) = {\phi_k \mathcal{N}(x^{(i)}; \mu_k, \Sigma_k) \over \sum_{j=1}^K \phi_j \mathcal{N}(x^{(i)}; \mu_j, \Sigma_j)}
    $$

    这一步等于是求解样本后验概率的期望，即对应EM算法的*Expectation*。

2.  重新调整质心以确定新的分布 ⇒ 估计每个子分布的参数 $\mu_k$ 和 $\Sigma_k$ ，以及多项分布的参数 $\phi_k$ 。由于已知样本数据点 $x^{(i)}$ 属于类别 $k$ 的概率，也可以看作该点的值有 $\gamma(i,k)x^{(i)}$ 这么多的部分是由类别 $k$ 所对应的子分布生成的；反过来可以知道类别 $k$ 对应的子分布生成的数据点可以表示为 $\gamma(1,k)x^{(1)},\dots,\gamma(N,k)x^{(N)}$ ；由于每个子分布都是高斯分布，可以得到其对应的参数值：

    $$
    \begin{align}
    \mu_k &= \frac{1}{N_k}\sum_{i=1}^N \gamma(i,k)x^{(i)} \\
    \Sigma_k &= \frac{1}{N_k}\sum_{i=1}^N \gamma(i,k)x^{(i)}(x^{(i)} - \mu_k)(x^{(i)} - \mu_k)^{\mathsf{T}}
    \end{align}
    $$

    其中， $N_k=\sum_{i=1}^N \gamma(i,k)$ 。同时，可以得到多项分布的参数值：

    $$
    \phi_k = \frac{N_k}{N}
    $$

    这一步实际是在用正常的最大似然估计方法估计相关参数，即对应EM算法的*Maximization*。

如果要深入理解EM算法，强烈推荐阅读PRML第九章节，以及相关Notes。

## 参考

- [Stanford CS229 Machine Learning](http://cs229.stanford.edu/notes/cs229-notes7b.pdf)
- [漫谈 Clustering (3): Gaussian Mixture Model](http://blog.pluskid.org/?p=39)
- PRML, 9. Mixture Models and EM
- Sean Borman, [The Expectation Maximization Algorithm A short tutorial](http://www.seanborman.com/publications/EM_algorithm.pdf)
- ChengXiang Zhai, [A Note on the Expectation-Maximization (EM) Algorithm](http://www.cs.ust.hk/~qyang/Teaching/537/PPT/em-note.pdf)
